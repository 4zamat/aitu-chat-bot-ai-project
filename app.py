import streamlit as st
from data_handler import load_data_from_csv
from chatbot_logic_v3 import create_sbert_embeddings, find_best_match_sbert
from llm_handler import generate_llm_answer

# Cache models to prevent reloading on every interaction
# @st.cache_resource ensures function runs once and caches result in memory
@st.cache_resource
def load_all_models():
    """
    Loads SBERT model, data, and creates embeddings once.
    Cached by Streamlit to avoid reloading on every message.
    """
    print("--- –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô (–í–´–ü–û–õ–ù–Ø–ï–¢–°–Ø –û–î–ò–ù –†–ê–ó) ---")
    DATA_FILE = "data/QA_cleaned.csv" 
    faq_data = load_data_from_csv(DATA_FILE)
    
    if faq_data is None:
        st.error("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏.")
        return None, None

    print("–°–æ–∑–¥–∞—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ—Ä–ø—É—Å –¥–ª—è SBERT...")
    combined_texts = [f"–í–æ–ø—Ä–æ—Å: {item.get('questions', '')} –û—Ç–≤–µ—Ç: {item.get('answers', '')}" for item in faq_data]
    
    context_embeddings = create_sbert_embeddings(combined_texts)
    
    if context_embeddings is None:
        st.error("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å SBERT –≤–µ–∫—Ç–æ—Ä—ã.")
        return None, None
    
    print("--- –ú–û–î–ï–õ–ò –ì–û–¢–û–í–´ ---")
    return faq_data, context_embeddings

# Load models once (cached)
faq_data, context_embeddings = load_all_models()

# Initialize UI and session state
st.title("ü§ñ FAQ-–ë–æ—Ç AITU (RAG v3.4)")
st.caption("–ù–∞ –±–∞–∑–µ SBERT, Gemini –∏ Streamlit")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []
    
# Initialize pending topic for disambiguation (Plan C)
if "pending_topic" not in st.session_state:
    st.session_state.pending_topic = None

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Main chat input handler
if prompt := st.chat_input("–°–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –æ AITU..."):
    
    # Add and display user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
        
    # Plan C: Check if we're waiting for clarification
    if st.session_state.pending_topic:
        # Combine clarification with pending topic
        print(f"–ü–õ–ê–ù C: –û–±—ä–µ–¥–∏–Ω—è—é '{prompt}' —Å —Ç–µ–º–æ–π '{st.session_state.pending_topic}'")
        final_prompt = f"{prompt} {st.session_state.pending_topic}"
        st.session_state.pending_topic = None
    else:
        final_prompt = prompt

    # Query expansion for short queries (< 3 words)
    if len(final_prompt.split()) < 3:
        print("... (Query Expansion)...")
        sbert_query = f"–í–æ–ø—Ä–æ—Å –ø–æ —Ç–µ–º–µ: {final_prompt}"
    else:
        sbert_query = final_prompt
    
    # Plan A: Run SBERT search
    print(f"SBERT –ò—â–µ—Ç –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{sbert_query}'")
    found_contexts_list = find_best_match_sbert(sbert_query, 
                                                context_embeddings, 
                                                faq_data)
    
    # Generate response (Plan A: RAG or Plan C: Disambiguation)
    with st.chat_message("assistant"):
        with st.spinner("–î—É–º–∞—é..."):
            if found_contexts_list:
                # Plan A: RAG - use original (non-expanded) prompt for LLM
                print("–ü–õ–ê–ù –ê: SBERT –Ω–∞—à–µ–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ó–∞–ø—É—Å–∫–∞—é RAG.")
                llm_answer = generate_llm_answer(prompt, found_contexts_list)
                st.markdown(llm_answer)
                
            else:
                # Plan C: Disambiguation - ask for clarification
                print("–ü–õ–ê–ù C: SBERT –Ω–µ –Ω–∞—à–µ–ª. –ó–∞–ø—Ä–∞—à–∏–≤–∞—é —É—Ç–æ—á–Ω–µ–Ω–∏–µ.")
                
                response_text = f"–Ø –≤–∏–∂—É, –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç —Ç–µ–º–∞: **'{prompt}'**. \n\n" \
                                "–ù–µ –º–æ–≥–ª–∏ –±—ã –≤—ã —É—Ç–æ—á–Ω–∏—Ç—å, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ —É–∑–Ω–∞—Ç—å? \n\n" \
                                "(–ù–∞–ø—Ä–∏–º–µ—Ä: *—Å—Ç–æ–∏–º–æ—Å—Ç—å*, *—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ*, *–¥–æ–∫—É–º–µ–Ω—Ç—ã* –∏ —Ç.–¥.)"
                
                st.markdown(response_text)
                
                # Store topic in memory for next interaction
                st.session_state.pending_topic = prompt

    # Save assistant response to chat history
    if 'llm_answer' in locals():
        st.session_state.messages.append({"role": "assistant", "content": llm_answer})
    elif 'response_text' in locals():
        st.session_state.messages.append({"role": "assistant", "content": response_text})   