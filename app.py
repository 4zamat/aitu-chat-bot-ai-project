import streamlit as st
from alem_llm_handler import generate_llm_answer
from chatbot_logic_alem import load_precomputed_data, find_best_match_alem

# Cache models to prevent reloading on every interaction
@st.cache_resource
def load_all_models():
    """
    Loads precomputed Alem embeddings once.
    Cached by Streamlit to avoid reloading on every message.
    """
    print("--- –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò –í–ï–ö–¢–û–†–û–í (ALEM) ---")
    texts, vectors, data = load_precomputed_data()
    
    if texts is None:
        st.error("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª alem_embeddings.pkl!")
        return None, None, None
    
    print("--- –ú–û–î–ï–õ–ò –ì–û–¢–û–í–´ (ALEM) ---")
    return texts, vectors, data

# Load models once (cached)
precomputed_texts, precomputed_vectors, faq_data = load_all_models()

# Initialize UI and session state
st.title("ü§ñ FAQ-–ë–æ—Ç AITU (Alem.ai RAG)")
st.caption("–ù–∞ –±–∞–∑–µ Alem Embedder, Reranker –∏ LLM")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []
    
# Initialize pending topic for disambiguation (Plan C)
if "pending_topic" not in st.session_state:
    st.session_state.pending_topic = None

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Main chat input handler
if prompt := st.chat_input("–°–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –æ AITU..."):
    
    # Add and display user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
        
    # Plan C: Check if we're waiting for clarification
    if st.session_state.pending_topic:
        # Combine clarification with pending topic
        final_prompt = f"{prompt} {st.session_state.pending_topic}"
        st.session_state.pending_topic = None
    else:
        final_prompt = prompt

    # Note: Query expansion removed - Reranker handles relevance ranking
    
    # Plan A: Run Alem search pipeline
    print(f"Alem-–ü–∞–π–ø–ª–∞–π–Ω: –ò—â—É –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{final_prompt}'")
    
    found_contexts_list = find_best_match_alem(
        final_prompt, 
        precomputed_texts, 
        precomputed_vectors, 
        faq_data
    )
    
    # Generate response (Plan A: RAG or Plan C: Disambiguation)
    with st.chat_message("assistant"):
        with st.spinner("–î—É–º–∞—é (Alem.ai)..."):
            
            if found_contexts_list:
                # Plan A: RAG - use original prompt for LLM
                print(f"–ü–õ–ê–ù –ê: Alem-–ü–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(found_contexts_list)} –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ó–∞–ø—É—Å–∫–∞—é RAG.")
                llm_answer = generate_llm_answer(prompt, found_contexts_list)
                st.markdown(llm_answer)
                
            else:
                # Plan C: Disambiguation - ask for clarification
                print("–ü–õ–ê–ù C: Alem-–ü–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª. –ó–∞–ø—Ä–∞—à–∏–≤–∞—é —É—Ç–æ—á–Ω–µ–Ω–∏–µ.")
                response_text = f"–Ø –≤–∏–∂—É, –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç —Ç–µ–º–∞: **'{prompt}'**. \n\n" \
                                "–ù–µ –º–æ–≥–ª–∏ –±—ã –≤—ã —É—Ç–æ—á–Ω–∏—Ç—å, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ —É–∑–Ω–∞—Ç—å?"
                st.markdown(response_text)
                # Store topic in memory for next interaction
                st.session_state.pending_topic = prompt

    # Save assistant response to chat history
    if 'llm_answer' in locals():
        st.session_state.messages.append({"role": "assistant", "content": llm_answer})
    elif 'response_text' in locals():
        st.session_state.messages.append({"role": "assistant", "content": response_text})